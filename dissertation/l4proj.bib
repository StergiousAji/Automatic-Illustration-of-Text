@misc{rise_video, 
    author={{Team Pepper}},
    year={2022},
    title={The rise of video content: A brief explainer}, 
    howpublished ={\url{https://www.peppercontent.io/blog/rise-of-video-content/}},
    note = {Accessed: 2023-02-15}
 } 

@misc{youtube, 
    author={YouTube},
    year={2023},
    title={YouTube}, 
    howpublished ={\url{https://www.youtube.com/}},
    note = {Accessed: 2023-02-20}
 } 

 @misc{parker,
    author={Andrew Parker},
    year={2022},
    title={{Automatic Videography of Audio Tracks of Songs}},
    howpublished={\url{https://github.com/AndrewParker770/Automatic-Videography-of-Audio-Tracks-of-Songs}},
    note = {Accessed: 2023-02-20}
}
 
@misc{benefits_of_mmv, 
    title={Exploring the perceived benefits of the process of multimodal video making in developing multiliteracies}, 
    url={https://scholarspace.manoa.hawaii.edu/handle/10125/44642}, 
    journal={ScholarSpace}, 
    publisher={University of Hawaii National Foreign Language Resource Center}, 
    author={Yeh, Hui-Chin}, 
    year={2018}, 
    month={Jun}
} 

@article{vark,
  title={VARK visual, aural/auditory, read/write, kinesthetic},
  author={Fleming, Neil D},
  journal={New Zealand: Bonwell Green Mountain Falls},
  year={2006}
}

 @misc{django, 
    author = {Django},
    year = {2005},
    title = {The web framework for perfectionists with deadlines},
    howpublished ={\url{https://www.djangoproject.com/}},
    note = {Accessed: 2023-02-14}
}

@misc{lrc,
    author = {Kuo (Djohan) Shiang-shiang},
    year = {2023},
    title = {LyRiCs (LRC) File Format},
    howpublished ={\url{https://docs.fileformat.com/misc/lrc/}},
    note = {Accessed: 2023-02-14},
}

@misc{pytube,
    author = {Ronnie Ghose, Taylor Fox Dahlin, Nick Ficano},
    year = {2022},
    title = {pytube},
    howpublished ={\url{https://github.com/pytube/pytube}},
    note = {Accessed: 2023-02-14},
}

@misc{shazamapi,
    author = {Numenorean},
    year = {2021},
    title = {ShazamAPI},
    howpublished ={\url{https://github.com/Numenorean/ShazamAPI}},
    note = {Accessed: 2023-02-14},
}

@misc{musixmatch,
    author = {MusixMatch},
    year = {2023},
    title = {MusixMatch Developer},
    howpublished ={\url{https://developer.musixmatch.com/}},
    note = {Accessed: 2023-02-14},
}

@misc{whisper, 
    title={Robust speech recognition via large-scale weak supervision}, 
    url={https://arxiv.org/abs/2212.04356}, 
    journal={arXiv.org}, 
    author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya}, 
    year={2022}, 
    month={Dec}
}

@misc{mclip,
    author = {Fredde Frallan},
    year = {2022},
    title = {Multilingual-CLIP},
    howpublished ={\url{https://github.com/FreddeFrallan/Multilingual-CLIP}},
    note = {Accessed: 2023-02-14},
}

@inproceedings{json,
    title={Foundations of JSON schema},
    author={Pezoa, Felipe and Reutter, Juan L and Suarez, Fernando and Ugarte, Mart{\'\i}n and Vrgo{\v{c}}, Domagoj},
    booktitle={Proceedings of the 25th International Conference on World Wide Web},
    pages={263--273},
    year={2016},
    organization={International World Wide Web Conferences Steering Committee}
}

@article{datta2001cbir,
    title={Content-based image retrieval systems},
    author={Datta, Ritendra and Wang, James Z},
    journal={IEEE Signal Processing Magazine},
    volume={18},
    number={4},
    pages={20--37},
    year={2001},
    publisher={IEEE}
}

@inproceedings{lavrenko2003lop,
    title={A model for learning the semantics of pictures},
    author={Lavrenko, Victor and Manmatha, R and Jeon, Jiwoon},
    booktitle={Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval},
    pages={187--194},
    year={2003},
    organization={ACM}
}

@inproceedings{rasiwasia2010cmmr,
    title={A new approach to cross-modal multimedia retrieval},
    author={Rasiwasia, Nikhil and Mahajan, Dhruv and Chen, Vivien and Takacs, Gabriella and Ahuja, Narendra},
    booktitle={Proceedings of the 18th ACM international conference on Multimedia},
    pages={251--260},
    year={2010},
    organization={ACM}
}

@inproceedings{gao2018tgam,
    title={Text-guided attention model for image captioning and visual question answering},
    author={Gao, Huaizu and Zhang, Jun and Mao, Jianfeng and Zhu, Jianfei and Song, Wen and Huang, Heng},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    pages={19--28},
    year={2018}
}

@inproceedings{karpathy2015dvsa,
    title={Deep visual-semantic alignments for generating image descriptions},
    author={Karpathy, Andrej and Fei-Fei, Li},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    pages={3128--3137},
    year={2015}
}

@inproceedings{vinyals2015nic,
    title={Show and tell: A neural image caption generator},
    author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    pages={3156--3164},
    year={2015}
}

@inproceedings{zhang2017stackgan,
    title={Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks},
    author={Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Wei, Ding},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    pages={5907--5915},
    year={2017}
}

@misc{radford2021clip,
    title={Learning transferable visual models from natural language supervision},
    author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Gohil, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
    year={2021},
    eprint={2103.00020},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{vaswani2017attention,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    booktitle={Advances in neural information processing systems},
    pages={5998--6008},
    year={2017}
}

@article{devlin2018bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

@article{radford2018gpt,
    title={Improving language understanding by generative pre-training},
    author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
    year={2018},
    publisher={OpenAI}
}

@article{dosovitskiy2020vit,
    title={An image is worth 16x16 words: Transformers for image recognition at scale},
    author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
    journal={arXiv preprint arXiv:2010.11929},
    year={2020}
}

@article{radford2019gpt2,
    title={Language models are unsupervised multitask learners},
    author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
    journal={OpenAI blog},
    volume={1},
    number={8},
    pages={9},
    year={2019}
}
